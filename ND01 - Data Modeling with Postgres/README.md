## Project

    A startup company called Sparkify wants to analyze the data they've been collecting on songs and user activity on their new music streaming app. The analytics team wants to read the data stored in the JSON log files which are generated by the mobile application and understand the user activity such as their next song preference.

Tools

     1. Postgresql Database.
     2. Python 
     3. Juptyer Notebook.

## Database
    
    Sparifkydb is created to perform the analytics on the music data to better understand the user's perference for the next song. For the same purpose, following DB objects are created,   

    ### Fact Table
    
        1. songplays - records in log data associated with song plays i.e. records with page NextSong
           Columns: songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent
       
    ### Dimension Table
    
        1. Users - users data are stored are stored in this table.
           Columns: user_id, first_name, last_name, gender, level
        2. Songs - This table stores all the data related to songs. 
           Columns: song_id, title, artist_id, year, duration
        3. Artists - Data related to Artists are stored in this table. 
           Columns: artist_id, name, location, latitude, longitude
        4. time - This data is related to time customer/user have listened to the song. 
           Columns: Columns: start_time, hour, day, week, month, year, weekday

## Scripts

    1. create_table.py to create the DB objects.
    2. sql_queries.py script has all the DML operations required for the project.
    3. etl.py script reads the data from the json files in the /data/song_data and /log_data stores them into the tables.
    4. etl.ipynb are step and step execution of the entire process.
    5. test.ipynb is used to verify the data that loaded into the table.

## Process
    
    The first steps is create the database "sparkifydb" and the tables in the Database. The etl.py process reads the data from the files in the song_data and log_data directory and stores the data into the tables we have just created. After the completion of the load process, data for the songsplay is extracted from the songs and artists tables based on the Song name, Artist, song's duration. One of the issue experience during the process is the duration had to be rounded and it was done using the below mentioned sql query. 
    
         SELECT song_id, artists.artist_id 
         FROM songs JOIN artists ON songs.artist_id = artists.artist_id 
         WHERE songs.title = %s 
           AND artists.name = %s           
           AND round(songs.duration) = round(%s)


## Execution Steps. 

    1. Open Terminal in Unix.
    2. Run the command "python create_tables.py"
        This process drops and recreated the database. 
    3. Run the etl.py script by executing the command "python etl.py"
         etl.py scripts extracts the data from the songs and log data from the json files and stores them into the respective Dimension tables. At the same time, populates the Fact table while sourcing the data from the thus loaded dimension tables. 
    4. Run the test.ipynd notebook to validate the the results from the process we just ran. 