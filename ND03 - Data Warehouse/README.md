## Project

    A startup company called Sparkify wants to analyze the data they've been collecting on songs and user activity on 
    their new music streaming app. The analytics team wants to read the data stored in the JSON log files which are 
    generated by the mobile application and understand the user activity such as their next song preference.

##Tools

     1. AWS Redshift Database.
     2. Python

## Database
    
    Sparifkydb (DWH) is created in Amazon Webservice to perform the analytics on the music data to better understand the user's perference for the next song.
    For the same purpose, following DB objects are created,   

    Fact Table
        1. songplays - records in log data associated with song plays i.e. records with page NextSong
           Columns: songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent
       
    Dimension Tables
        1. Users - users data are stored are stored in this table.
           Columns: user_id, first_name, last_name, gender, level
        2. Songs - This table stores all the data related to songs. 
           Columns: song_id, title, artist_id, year, duration
        3. Artists - Data related to Artists are stored in this table. 
           Columns: artist_id, name, location, latitude, longitude
        4. time - This data is related to time customer/user have listened to the song. 
           Columns: Columns: start_time, hour, day, week, month, year, weekday
    
    Staging Tables
        1.  staging_events 
            Columns: artist, auth, firstName, gender, itemInSession, lastName, length, level, location, method, page, registration, sessionId, song, status, ts, userAgent, userId  
        2. staging_songs
            Columns: num_songs, artist_id, artist_latitude, artist_longitude, artist_location, artist_name, song_id, title, duration, year
            
## Scripts

    1. create_table.py to create the DB objects.
    2. sql_queries.py script has all the DML operations required for the project.
    3. etl.py script reads the data from the json files that are stored on AWS S3 buckets 
        (a). s3://udacity-dend/log_data
        (b). s3://udacity-dend/song_data
        
## Process
    
    The first steps is create the database "sparkifydb" and the tables in the Database. 
    The etl.py process reads the data from the files from the s3 buckets and loads the data into the staying table staging_events and staging_songs. 
    After the data load process is complete, etl.py scripts sources the data for the Fact and Dimension Tables from the staying tables. 
    
    SQL Queries used 
        (a). For Songplay
            SELECT TIMESTAMP 'epoch' + ts/1000 *INTERVAL '1 second', userId, level, song_id, artist_id,  sessionId, location, useragent 
                              FROM   staging_songs, staging_events 
                              WHERE  title = song 
                              AND    artist = artist_name
                              AND    page = 'NextSong'
    
        (b). For Users 
            SELECT userId, firstName, lastname, gender, level 
            FROM    staging_events
        
        (c). For Songs
             SELECT song_id, title, artist_id, year, duration 
             FROM staging_songs
        
        (d). For Artists 
             SELECT artist_id, artist_name, artist_location, artist_latitude, artist_longitude
             FROM staging_songs
             
        (e). For Time
             SELECT  DISTINCT(start_time)                AS start_time, \
                     EXTRACT(hour FROM start_time)       AS hour, \
                     EXTRACT(day FROM start_time)        AS day, \
                     EXTRACT(week FROM start_time)       AS week, \
                     EXTRACT(month FROM start_time)      AS month, \
                     EXTRACT(year FROM start_time)       AS year, \
                     EXTRACT(dayofweek FROM start_time)  as weekday \
                     FROM songplay

## Execution Steps. 

    1. Create AWS resouces IAM, Redshift DB and S3 buckets. 
        Note: Resources can be created either manually or using IAC (Infrastructure As Code). AWS configure is beyound the scope of this project. 
    2. Update the dwh.cfg file with all the configuration settings. 
    3. Run the command "python create_tables.py"
        This process drops and recreated the database tables. 
    3. Run the etl.py script by executing the command "python etl.py"
         etl.py scripts extracts the data elements from the songs and log files (json files) which are stored on AWS s3 buckets and stores them into the staging tables.
         etl.py script reads the data from these newly created DB and populates the Fact and Dimension tables. 
         Note: Data import is completed using the COPY command.
    4. Run the test.ipynd notebook to validate the the results from the process we just ran. 
    
 