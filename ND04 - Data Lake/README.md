## Project

    A music streaming startup company, Sparkify wants to move the data from Data Warehouse to DataLake to analyze the data they've been collecting 
    on songs and user activity on their new music streaming app. The Data/analytics team wants to read the data stored in the JSON log files (s3 buckets in AWS)
    which are generated by the mobile application.
    
##Tools

     1. Pyspark.
     2. AWS S3 Service
     3. Python

## Design
    Pyspark process reads both log and songs data from the s3 buckets on AWS cloud provider and parse them into Virtual tables (dataframes) for analysis
    and stores them parquet file formats onto s3 buckets.
  
    Below are the virtual tables design 

    Fact Table
        1. songplays - records in log data associated with song plays i.e. records with page NextSong
           Columns: songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent
       
    Dimension Tables
        1. Users - users data are stored are stored in this table.
           Columns: user_id, first_name, last_name, gender, level
        2. Songs - This table stores all the data related to songs. 
           Columns: song_id, title, artist_id, year, duration
        3. Artists - Data related to Artists are stored in this table. 
           Columns: artist_id, name, location, latitude, longitude
        4. time - This data is related to time customer/user have listened to the song. 
           Columns: Columns: start_time, hour, day, week, month, year, weekday
           
## Process
    
    The first steps is read the data from the below s3 buckets. The etl.py process reads the data from the files from the s3 buckets and loads the data into the staying table staging_events and staging_songs. 
    After the data load process is complete, etl.py scripts sources the data into the Fact and Dimension Tables from the staying tables. 
        
## Execution Steps. 

    1. Update the dl.cfg file with all the credentials. 
    2. Run the command "python etl.py" scripts
        This process reads the data,  parse anc create parquet files. 
 